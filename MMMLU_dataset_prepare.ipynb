{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ee33aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import Value, Dataset\n",
    "\n",
    "DEFAULT_LETTER_ORDER = np.array([\"A\", \"B\", \"C\", \"D\"])\n",
    "def to_mmmlu_format(mmlu_dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Convert the format of English mmlu to mmmlu.\n",
    "    \"\"\"\n",
    "    for col_name in mmlu_dataset.features:\n",
    "        mmlu_dataset = mmlu_dataset.rename_column(col_name, col_name.capitalize())\n",
    "\n",
    "    # Add options columns\n",
    "    choice_arr = np.array(mmlu_dataset['Choices'])\n",
    "    for idx, letter in enumerate(DEFAULT_LETTER_ORDER):\n",
    "        mmlu_dataset = mmlu_dataset.add_column(letter, choice_arr[:, idx])\n",
    "    mmlu_dataset = mmlu_dataset.remove_columns('Choices')\n",
    "\n",
    "    # From number answer cols to letter answer cols\n",
    "    mmlu_dataset = mmlu_dataset.cast_column('Answer', Value('string'))\n",
    "    mmlu_dataset = mmlu_dataset.map(lambda x: {\"Answer\": DEFAULT_LETTER_ORDER[int(x['Answer'])]})\n",
    "\n",
    "    # Add Question id in subtask for convenience\n",
    "    df = mmlu_dataset.to_pandas()\n",
    "    df['Question id in subtask'] = df.groupby(\"Subject\").cumcount()\n",
    "    columns = list(df.columns)\n",
    "    question_id_col_name = columns.pop()\n",
    "    columns.insert(0, question_id_col_name)\n",
    "    df = df[columns]\n",
    "    mmlu_dataset = Dataset.from_pandas(df)\n",
    "    return mmlu_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "49c1cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# [English, JA_JP]\n",
    "lang_list = [\"English\", \"JA_JP\"]\n",
    "curr_langauage = lang_list[0]\n",
    "if curr_langauage == 'English':\n",
    "    mmmlu_ds = load_dataset('cais/mmlu', 'all', split='test')\n",
    "    mmmlu_ds = to_mmmlu_format(mmmlu_ds)\n",
    "else:\n",
    "    mmmlu_ds = load_dataset(\"openai/MMMLU\", curr_langauage, split='test')\n",
    "    mmmlu_ds = mmmlu_ds.rename_column('Unnamed: 0', \"Question id in subtask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "abd42850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hendrycks/test/blob/master/categories.py\n",
    "subcategories = {\n",
    "    \"abstract_algebra\": [\"math\"],\n",
    "    \"anatomy\": [\"health\"],\n",
    "    \"astronomy\": [\"physics\"],\n",
    "    \"business_ethics\": [\"business\"],\n",
    "    \"clinical_knowledge\": [\"health\"],\n",
    "    \"college_biology\": [\"biology\"],\n",
    "    \"college_chemistry\": [\"chemistry\"],\n",
    "    \"college_computer_science\": [\"computer science\"],\n",
    "    \"college_mathematics\": [\"math\"],\n",
    "    \"college_medicine\": [\"health\"],\n",
    "    \"college_physics\": [\"physics\"],\n",
    "    \"computer_security\": [\"computer science\"],\n",
    "    \"conceptual_physics\": [\"physics\"],\n",
    "    \"econometrics\": [\"economics\"],\n",
    "    \"electrical_engineering\": [\"engineering\"],\n",
    "    \"elementary_mathematics\": [\"math\"],\n",
    "    \"formal_logic\": [\"philosophy\"],\n",
    "    \"global_facts\": [\"other\"],\n",
    "    \"high_school_biology\": [\"biology\"],\n",
    "    \"high_school_chemistry\": [\"chemistry\"],\n",
    "    \"high_school_computer_science\": [\"computer science\"],\n",
    "    \"high_school_european_history\": [\"history\"],\n",
    "    \"high_school_geography\": [\"geography\"],\n",
    "    \"high_school_government_and_politics\": [\"politics\"],\n",
    "    \"high_school_macroeconomics\": [\"economics\"],\n",
    "    \"high_school_mathematics\": [\"math\"],\n",
    "    \"high_school_microeconomics\": [\"economics\"],\n",
    "    \"high_school_physics\": [\"physics\"],\n",
    "    \"high_school_psychology\": [\"psychology\"],\n",
    "    \"high_school_statistics\": [\"math\"],\n",
    "    \"high_school_us_history\": [\"history\"],\n",
    "    \"high_school_world_history\": [\"history\"],\n",
    "    \"human_aging\": [\"health\"],\n",
    "    \"human_sexuality\": [\"culture\"],\n",
    "    \"international_law\": [\"law\"],\n",
    "    \"jurisprudence\": [\"law\"],\n",
    "    \"logical_fallacies\": [\"philosophy\"],\n",
    "    \"machine_learning\": [\"computer science\"],\n",
    "    \"management\": [\"business\"],\n",
    "    \"marketing\": [\"business\"],\n",
    "    \"medical_genetics\": [\"health\"],\n",
    "    \"miscellaneous\": [\"other\"],\n",
    "    \"moral_disputes\": [\"philosophy\"],\n",
    "    \"moral_scenarios\": [\"philosophy\"],\n",
    "    \"nutrition\": [\"health\"],\n",
    "    \"philosophy\": [\"philosophy\"],\n",
    "    \"prehistory\": [\"history\"],\n",
    "    \"professional_accounting\": [\"other\"],\n",
    "    \"professional_law\": [\"law\"],\n",
    "    \"professional_medicine\": [\"health\"],\n",
    "    \"professional_psychology\": [\"psychology\"],\n",
    "    \"public_relations\": [\"politics\"],\n",
    "    \"security_studies\": [\"politics\"],\n",
    "    \"sociology\": [\"culture\"],\n",
    "    \"us_foreign_policy\": [\"politics\"],\n",
    "    \"virology\": [\"health\"],\n",
    "    \"world_religions\": [\"philosophy\"],\n",
    "}\n",
    "\n",
    "categories = {\n",
    "    \"STEM\": [\"physics\", \"chemistry\", \"biology\", \"computer science\", \"math\", \"engineering\"],\n",
    "    \"humanities\": [\"history\", \"philosophy\", \"law\"],\n",
    "    \"social sciences\": [\"politics\", \"culture\", \"economics\", \"geography\", \"psychology\"],\n",
    "    \"other (business, health, misc.)\": [\"other\", \"business\", \"health\"],\n",
    "}\n",
    "\n",
    "# Choose the first subtask in the subcategory\n",
    "subtasks = {}\n",
    "chosen_subtasks = []\n",
    "for subtask, task_subcategory in subcategories.items():\n",
    "    curr_category = task_subcategory[0]\n",
    "    if curr_category not in subtasks:\n",
    "        subtasks[curr_category] = []\n",
    "        chosen_subtasks.append(subtask)\n",
    "    subtasks[curr_category].append(subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8b3c09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually select. Prefer professional over high school\n",
    "target_subtasks = ['abstract_algebra', 'virology', 'astronomy', 'marketing', 'college_biology', 'college_chemistry', 'machine_learning',\n",
    "    'econometrics', 'electrical_engineering', 'philosophy', 'global_facts', 'prehistory',\n",
    "    'high_school_geography', 'security_studies', 'professional_psychology', 'sociology', 'jurisprudence'\n",
    "]\n",
    "\n",
    "subtasks = {}\n",
    "for subtask, task_subcategory in subcategories.items():\n",
    "    curr_category = task_subcategory[0]\n",
    "    if curr_category not in subtasks:\n",
    "        subtasks[curr_category] = []\n",
    "    subtasks[curr_category].append(subtask)\n",
    "\n",
    "chosen_subtasks = target_subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "39f89fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_algebra',\n",
       " 'virology',\n",
       " 'astronomy',\n",
       " 'marketing',\n",
       " 'college_biology',\n",
       " 'college_chemistry',\n",
       " 'machine_learning',\n",
       " 'econometrics',\n",
       " 'electrical_engineering',\n",
       " 'philosophy',\n",
       " 'global_facts',\n",
       " 'prehistory',\n",
       " 'high_school_geography',\n",
       " 'security_studies',\n",
       " 'professional_psychology',\n",
       " 'sociology',\n",
       " 'jurisprudence']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fc1fae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.disable_progress_bar()\n",
    "def sample_first_n_data_from_subtask(ds: Dataset, target_subtasks: list[str], sample_first_n: int = 100) -> Dataset:\n",
    "    subtask_set = set(set(target_subtasks))\n",
    "    sampled_ds = ds.filter(lambda x: x['Subject'] in subtask_set and x['Question id in subtask'] < sample_first_n)\n",
    "    return sampled_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5f178d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Question id in subtask', 'Question', 'Subject', 'Answer', 'A', 'B', 'C', 'D'],\n",
       "    num_rows: 1700\n",
       "})"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmmlu_subset = sample_first_n_data_from_subtask(mmmlu_ds, chosen_subtasks)\n",
    "mmmlu_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e56cfd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# common.py\n",
    "\n",
    "# The original base prompt\n",
    "QUERY_TEMPLATE_MULTICHOICE = \"\"\"\n",
    "Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\n",
    "\n",
    "{Question}\n",
    "\n",
    "A) {A}\n",
    "B) {B}\n",
    "C) {C}\n",
    "D) {D}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Base prompt with variable output format\n",
    "BASE_TEMPLATE_FREE = \"\"\"\n",
    "Answer the following multiple choice question. {Output_format} Think step by step before answering.\n",
    "\n",
    "{Question}\n",
    "\n",
    "A) {A}\n",
    "B) {B}\n",
    "C) {C}\n",
    "D) {D}\n",
    "\"\"\".strip()\n",
    "\n",
    "# The sentecnes of base prompt\n",
    "BASE_TASK = \"Answer the following multiple choice question.\"\n",
    "BASE_OUTPUT_FORMAT = \"The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD.\"\n",
    "BASE_INSTRUCTION = \"Think step by step before answering.\"\n",
    "\n",
    "\n",
    "JSON_ANSWER_OUTPUT_FORMAT = \"\"\"The last line of your response should be of the following format:\n",
    "'\n",
    "{\n",
    "    \"Answer\": \"$LETTER\"\n",
    "}\n",
    "'\n",
    "where LETTER is one of ABCD. Namely, a JSON format for the output is required.\n",
    "Each value in JSON should be single line.\n",
    "\"\"\".strip()\n",
    "\n",
    "JSON_FULL_OUTPUT_FORMAT = \"\"\"Your response should be of the following format:\n",
    "'\n",
    "{\n",
    "    \"Reasoning\": ...,\n",
    "    \"Answer\": \"$LETTER\"\n",
    "}\n",
    "'\n",
    "where LETTER is one of ABCD. Namely, a JSON format for the output is required.\n",
    "Each value in JSON should be single line.\n",
    "\"\"\".strip()\n",
    "\n",
    "XML_ANSWER_OUTPUT_FORMAT = \"\"\"The last line of your response should be of the following format:\n",
    "'\n",
    "<root>\n",
    "    <Answer> $LETTER </Answer>\n",
    "</root>\n",
    "'\n",
    "where LETTER is one of ABCD. Namely, a XML format for the output is required.\n",
    "\"\"\".strip()\n",
    "\n",
    "XML_FULL_OUTPUT_FORMAT = \"\"\"Your response should be of the following format:\n",
    "'\n",
    "<root>\n",
    "    <Reasoning> ... </Reasoning>\n",
    "    <Answer> $LETTER </Answer>\n",
    "</root>\n",
    "'\n",
    "where LETTER is one of ABCD. Namely, a XML format for the output is required.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Draft Json. Not confirm\n",
    "JSON_TEMPLATE_DICT = {\n",
    "    \"Task\": \"\",\n",
    "    \"Output_format\": \"\",\n",
    "    \"Instruction\": \"\",\n",
    "    \"Question\": \"\",\n",
    "    \"Options\": {\n",
    "        \"A\": \"\",\n",
    "        \"B\": \"\",\n",
    "        \"C\": \"\",\n",
    "        \"D\": \"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fill task and instruction from base prompt\n",
    "base_json_dict = copy.deepcopy(JSON_TEMPLATE_DICT)\n",
    "base_json_dict['Task'] = BASE_TASK\n",
    "base_json_dict['Instruction'] = BASE_INSTRUCTION\n",
    "\n",
    "DEFAULT_LETTER_ORDER = np.array([\"A\", \"B\", \"C\", \"D\"])\n",
    "DEFAULT_ORDER = np.arange(0, len(DEFAULT_LETTER_ORDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d0d18f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "47378d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import html\n",
    "\n",
    "def get_xml_input(common_query_filling: dict, options_filling: dict) -> str:\n",
    "    '''\n",
    "    common_query_filling - question and output_format\n",
    "    options_filling - options and their contents\n",
    "    '''\n",
    "    xml_root = ET.Element(\"root\")\n",
    "    task_tag = ET.SubElement(xml_root, 'Task')\n",
    "    task_tag.text = BASE_TASK\n",
    "\n",
    "    output_tag = ET.SubElement(xml_root, \"Output_format\")\n",
    "    output_tag.text = common_query_filling['Output_format']\n",
    "\n",
    "    instruction_tag = ET.SubElement(xml_root, \"Instruction\")\n",
    "    instruction_tag.text = BASE_INSTRUCTION\n",
    "\n",
    "    question_tag = ET.SubElement(xml_root, \"Question\")\n",
    "    question_tag.text = common_query_filling['Question']\n",
    "\n",
    "    options_tag = ET.SubElement(xml_root, \"Options\")\n",
    "    for opt_id in DEFAULT_LETTER_ORDER:\n",
    "        opt_id = opt_id.upper()\n",
    "        opt_tag = ET.SubElement(options_tag, opt_id)\n",
    "        opt_tag.text = options_filling[opt_id]\n",
    "\n",
    "    ET.indent(xml_root)\n",
    "    xml_query = ET.tostring(xml_root, encoding='unicode')\n",
    "    return xml_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6f1ec39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last line of your response should be of the following format:\n",
      "'\n",
      "{\n",
      "    \"Answer\": \"$LETTER\"\n",
      "}\n",
      "'\n",
      "where LETTER is one of ABCD. Namely, a JSON format for the output is required.\n",
      "Each value in JSON should be single line.\n"
     ]
    }
   ],
   "source": [
    "print(JSON_ANSWER_OUTPUT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "77dd1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class InputFormat(Enum):\n",
    "    BASE = \"free text\"\n",
    "    JSON = \"json\"\n",
    "    XML = \"xml\"\n",
    "\n",
    "class OutputFormat(Enum):\n",
    "    BASE = \"free text\"\n",
    "    JSON_ANSWER = \"json answer\"\n",
    "    JSON_FULL = \"json full\"\n",
    "    XML_ANSWER = \"xml answer only\"\n",
    "    XML_FULL = \"xml full\"\n",
    "\n",
    "\n",
    "class ShuffleMethod(Enum):\n",
    "    DEFAULT = \"default\"\n",
    "    REVERSE = \"reverse\"\n",
    "    LONGEST_FIRST = \"longest-first\"\n",
    "    SHORTEST_FIRST = \"shortest-first\"\n",
    "\n",
    "    MOST_KANA = \"Japanese-kana-most\"\n",
    "    FEWEST_KANA = \"Japanese-kana-fewest\"\n",
    "\n",
    "    # Not use\n",
    "    # GOLD_A = \"gold A\"          # Always Put the answer in the option A\n",
    "    # GOLD_B = \"gold B\"\n",
    "    # GOLD_C = \"gold C\"\n",
    "    # GOLD_D = \"gold D\"\n",
    "\n",
    "output_format_dict = {\n",
    "    OutputFormat.BASE: BASE_OUTPUT_FORMAT,\n",
    "    OutputFormat.JSON_ANSWER: JSON_ANSWER_OUTPUT_FORMAT,\n",
    "    OutputFormat.JSON_FULL: JSON_FULL_OUTPUT_FORMAT,\n",
    "    OutputFormat.XML_ANSWER: XML_ANSWER_OUTPUT_FORMAT,\n",
    "    OutputFormat.XML_FULL: XML_FULL_OUTPUT_FORMAT,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5af60c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShuffleMethod.DEFAULT.value == \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b7ac599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Task\": \"Answer the following multiple choice question.\",\n",
      "    \"Output_format\": \"\",\n",
      "    \"Instruction\": \"Think step by step before answering.\",\n",
      "    \"Question\": \"\",\n",
      "    \"Options\": {\n",
      "        \"A\": \"\",\n",
      "        \"B\": \"\",\n",
      "        \"C\": \"\",\n",
      "        \"D\": \"\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Get Json string\n",
    "query_json_string = json.dumps(base_json_dict, ensure_ascii=False, indent=4)\n",
    "print(query_json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "\n",
    "DEFAULT_MAPPING = {str(letter):str(letter) for letter in DEFAULT_LETTER_ORDER}\n",
    "\n",
    "# hiragana \\u3040 - \\u309F\n",
    "# katagana \\u30A0 - \\u30FF\n",
    "# katagana extension \\u31F0-\\u31FF\n",
    "kana_pattern = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u31F0-\\u31FF]+')\n",
    "\n",
    "def get_option_mapping(option_contents, shuffle_method: ShuffleMethod = ShuffleMethod.DEFAULT, output_format: OutputFormat = OutputFormat.BASE) -> tuple[dict[str, str], dict[str, str]]:\n",
    "    '''\n",
    "    Retrieve a dictionary that could map original position to new position\n",
    "    Return: (original_to_shuffled, shuffled_to_original)\n",
    "    '''\n",
    "    # Temp\n",
    "    original_to_shuffled = DEFAULT_MAPPING\n",
    "    if shuffle_method is ShuffleMethod.REVERSE:\n",
    "        original_to_shuffled = {str(idx):str(to_idx) for idx, to_idx in zip(DEFAULT_LETTER_ORDER, np.flip(DEFAULT_LETTER_ORDER))}\n",
    "    elif shuffle_method in [ShuffleMethod.LONGEST_FIRST, ShuffleMethod.SHORTEST_FIRST]:\n",
    "        if shuffle_method is ShuffleMethod.LONGEST_FIRST:\n",
    "            sorted_source = np.argsort(-np.char.str_len(option_contents))\n",
    "        else:\n",
    "            sorted_source = np.argsort(np.char.str_len(option_contents))\n",
    "        original_to_shuffled = {str(DEFAULT_LETTER_ORDER[source_idx]):str(DEFAULT_LETTER_ORDER[pos_idx]) for pos_idx, source_idx in enumerate(sorted_source)}\n",
    "    elif shuffle_method in [ShuffleMethod.MOST_KANA, ShuffleMethod.FEWEST_KANA]:\n",
    "        kana_counts = np.array([sum(len(kanas) for kanas in kana_pattern.findall(option_content)) for option_content in option_contents])\n",
    "        if shuffle_method is ShuffleMethod.MOST_KANA:\n",
    "            sorted_source = np.argsort(-kana_counts)\n",
    "        else:\n",
    "            sorted_source = np.argsort(kana_counts)\n",
    "        original_to_shuffled = {str(DEFAULT_LETTER_ORDER[source_idx]):str(DEFAULT_LETTER_ORDER[pos_idx]) for pos_idx, source_idx in enumerate(sorted_source)}\n",
    "    return original_to_shuffled\n",
    "\n",
    "\n",
    "def get_output_format(output_format_type: OutputFormat = OutputFormat.BASE) -> str:\n",
    "    return output_format_dict[output_format_type]\n",
    "\n",
    "# Base Prompt\n",
    "def get_query_shuffle_pair(curr_question : str,\n",
    "                           shuffle_method: ShuffleMethod = ShuffleMethod.DEFAULT,\n",
    "                           input_format_type: InputFormat = InputFormat.BASE,\n",
    "                           output_format_type: OutputFormat = OutputFormat.BASE):\n",
    "    output_format = get_output_format(output_format_type)\n",
    "    options = np.array([curr_question[opt] for opt in DEFAULT_LETTER_ORDER])\n",
    "    original_to_shuffled = get_option_mapping(options, shuffle_method)\n",
    "\n",
    "    common_query_filling = {'Question': curr_question['Question'].strip(), \"Output_format\": output_format}\n",
    "\n",
    "    # {shuffled_pos: content}\n",
    "    option_filling = {original_to_shuffled[opt]: curr_question[opt].strip() for opt in DEFAULT_LETTER_ORDER}\n",
    "\n",
    "    # Ensure the order. Sort to A -> D\n",
    "    option_filling = dict(sorted(option_filling.items()))\n",
    "    if input_format_type is InputFormat.BASE:\n",
    "        common_query_filling.update(option_filling)\n",
    "        query = BASE_TEMPLATE_FREE\n",
    "        result =  (query.format_map(common_query_filling), original_to_shuffled)\n",
    "    elif input_format_type is InputFormat.JSON:\n",
    "        query = copy.deepcopy(base_json_dict)\n",
    "        option_wrap = {\"Options\":  option_filling}\n",
    "        query.update(common_query_filling)\n",
    "        query.update(option_wrap)\n",
    "        json_query = json.dumps(query, ensure_ascii=False, indent=4)\n",
    "        result =  (json_query, original_to_shuffled)\n",
    "    elif input_format_type is InputFormat.XML:\n",
    "        xml_query = get_xml_input(common_query_filling, option_filling)\n",
    "        result = (xml_query, original_to_shuffled)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test concept\n",
    "for task_i, curr_subtask in enumerate(chosen_subtasks):\n",
    "    curr_ds = mmmlu_subset.filter(lambda x: x['Subject'] == curr_subtask)\n",
    "    shuffle_method = ShuffleMethod.DEFAULT\n",
    "    input_format = InputFormat.BASE\n",
    "    output_format = OutputFormat.BASE\n",
    "    query_list = []\n",
    "    for subtask_question_idx in range(100):\n",
    "        curr_query, curr_orig_to_shuffled = get_query_shuffle_pair(curr_ds[subtask_question_idx],\n",
    "                                                                                        shuffle_method=shuffle_method,\n",
    "                                                                                        input_format_type=input_format,\n",
    "                                                                                        output_format_type=output_format)\n",
    "        orig_ans = curr_ds[subtask_question_idx]['Answer']\n",
    "        curr_ans = curr_orig_to_shuffled[orig_ans] # Get the new position of answer\n",
    "        query_dict = {\n",
    "            \"Question id in subtask\": curr_ds[subtask_question_idx]['Question id in subtask'],\n",
    "            \"Shuffle method\": shuffle_method.value,\n",
    "            \"Original to shuffled\": curr_orig_to_shuffled,\n",
    "            \"Input format\": input_format.value,\n",
    "            \"Output format\": output_format.value,\n",
    "            \"Language\": curr_langauage,\n",
    "            \"Query\": curr_query,\n",
    "            \"Original correct answer\": orig_ans,\n",
    "            \"Shuffled correct answer\": curr_ans\n",
    "        }\n",
    "        query_list.append({\"Query\": curr_query, \"Original to shuffled\": curr_orig_to_shuffled, \"Original correct answer\": orig_ans, \"Shuffled correct answer\": curr_ans})\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0d4eacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Langsmith\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = \"nlp_final\"\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9384281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ea59280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content=\"Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\\n\\nStatement 1 | Every ideal in a ring is a subring of the ring. Statement 2 | Every subring of every ring is an ideal of the ring.\\n\\nA) True, True\\nB) False, False\\nC) True, False\\nD) False, True\", additional_kwargs={}, response_metadata={})]\n",
      "Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\n",
      "\n",
      "Statement 1 | Every ideal in a ring is a subring of the ring. Statement 2 | Every subring of every ring is an ideal of the ring.\n",
      "\n",
      "A) True, True\n",
      "B) False, False\n",
      "C) True, False\n",
      "D) False, True\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# language: The language to translate text into\n",
    "# text: The text to translate\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"text\": curr_query})\n",
    "print(prompt)\n",
    "print(curr_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7de81e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "037bd1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's analyze each statement.\n",
      "Statement 1: Every ideal in a ring is a subring of the ring.\n",
      "For a subset $I$ of a ring $R$ to be an ideal, it must satisfy:\n",
      "1. $I$ is a subgroup of $R$ under addition.\n",
      "2. For any $r \\in R$ and $x \\in I$, $rx \\in I$ and $xr \\in I$.\n",
      "For a subset $S$ of a ring $R$ to be a subring, it must satisfy:\n",
      "1. $S$ is a subgroup of $R$ under addition.\n",
      "2. $S$ is closed under multiplication.\n",
      "\n",
      "An ideal $I$ must be a subgroup under addition, but it is not necessarily closed under multiplication. For example, consider the ring $\\mathbb{Z}$ and the ideal $2\\mathbb{Z}$. $2\\mathbb{Z}$ is an ideal of $\\mathbb{Z}$, but $2 \\in 2\\mathbb{Z}$ and $2 \\cdot 2 = 4 \\in 2\\mathbb{Z}$. If we consider the ideal $2\\mathbb{Z}$ in $\\mathbb{Z}$, $2\\mathbb{Z}$ is a subring of $\\mathbb{Z}$ as well. However, consider the ring $\\mathbb{Z}_6$ and the ideal $I = \\{0, 2, 4\\}$. $I$ is an ideal of $\\mathbb{Z}_6$ and is a subring of $\\mathbb{Z}_6$ because $2 \\cdot 2 = 4 \\in I$, $2 \\cdot 4 = 8 \\equiv 2 \\pmod{6} \\in I$, $4 \\cdot 4 = 16 \\equiv 4 \\pmod{6} \\in I$. However, if $I$ is an ideal, it is not always a subring. For $I$ to be a subring, it must contain the multiplicative identity.\n",
      "For example, consider the ring $\\mathbb{Z}$ and the ideal $2\\mathbb{Z}$. The multiplicative identity $1 \\notin 2\\mathbb{Z}$, so $2\\mathbb{Z}$ is not a subring.\n",
      "Thus, statement 1 is false.\n",
      "\n",
      "Statement 2: Every subring of every ring is an ideal of the ring.\n",
      "This statement is false. Consider the ring $\\mathbb{Z}$ and the subring $S = \\mathbb{Z}$. Let $R = \\mathbb{Z}$. $S = \\mathbb{Z}$ is a subring of $R = \\mathbb{Z}$.\n",
      "Let $r \\in R$ and $x \\in S$. Then $r \\in \\mathbb{Z}$ and $x \\in \\mathbb{Z}$. Since $rx \\in \\mathbb{Z}$ and $xr \\in \\mathbb{Z}$, $rx \\in S$ and $xr \\in S$. So $\\mathbb{Z}$ is an ideal of $\\mathbb{Z}$. However, consider the ring $\\mathbb{Z}_6$ and the subring $S = \\{0, 1\\}$. $2 \\in \\mathbb{Z}_6$ and $1 \\in S$. $2 \\cdot 1 = 2 \\notin S$. Therefore, $S$ is not an ideal of $\\mathbb{Z}_6$. So statement 2 is false.\n",
      "\n",
      "Answer: B\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "eb51d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From common.py\n",
    "def normalize_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize the response by removing markdown and LaTeX formatting that may prevent a match.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        response.replace(\"**\", \"\")\n",
    "        .replace(\"$\\\\boxed{\", \"\")\n",
    "        .replace(\"}$\", \"\")\n",
    "        .replace(\"\\\\$\", \"\")\n",
    "        .replace(\"$\\\\text{\", \"\")\n",
    "        .replace(\"$\", \"\")\n",
    "        .replace(\"\\\\mathrm{\", \"\")\n",
    "        .replace(\"\\\\{\", \"\")\n",
    "        .replace(\"\\\\text\", \"\")\n",
    "        .replace(\"\\\\(\", \"\")\n",
    "        .replace(\"\\\\mathbf{\", \"\")\n",
    "        .replace(\"{\", \"\")\n",
    "        .replace(\"\\\\boxed\", \"\")\n",
    "    )\n",
    "\n",
    "MULTILINGUAL_ANSWER_REGEXES = [\n",
    "    \"Answer\\s*:\",\n",
    "    \"Answer\\s*:​​​​​​\",  # Korean invisible character\n",
    "    \"উত্তর\\s*:\",\n",
    "    \"उत्तर\\s*:\",\n",
    "    \"উত্তরঃ\",\n",
    "    \"উত্তর\\s*:\",\n",
    "    \"Antwort\\s*:\",\n",
    "    \"답변\\s*:\",\n",
    "    \"정답\\s*:\",\n",
    "    \"답\\s*:\",\n",
    "    \"答案\\s*：\",\n",
    "    \"答案\\s*:\",\n",
    "    \"答\\s*：\",\n",
    "    \"答\\s*:\",\n",
    "    \"答复\\s*：\",\n",
    "    \"答曰\\s*：\",\n",
    "    \"الإجابة:\",\n",
    "    \"الجواب:\",\n",
    "    \"إجابة:\",\n",
    "    \"الإجابة النهائية:\",\n",
    "    \"الإجابة الصحيحة:\",\n",
    "    \"الإجابة الصحيحة هي:\",\n",
    "    \"الإجابة هي:\",\n",
    "    \"الجواب النهائي:\",\n",
    "    \"Respuesta\\s*:\",\n",
    "    \"Risposta\\s*:\",\n",
    "    \"答え\\s*:\",\n",
    "    \"答え\\s*：\",\n",
    "    \"回答\\s*:\",\n",
    "    \"回答\\s*：\",\n",
    "    \"解答\\s*:\",\n",
    "    \"Jawaban\\s*:\",\n",
    "    \"Réponse\\s*:\",\n",
    "    \"Resposta\\s*:\",\n",
    "    \"Jibu\\s*:\",\n",
    "    \"Idahun\\s*:\",\n",
    "    \"Ìdáhùn\\s*:\",\n",
    "    \"Idáhùn\\s*:\",\n",
    "    \"Àmọ̀nà\\s*:\",\n",
    "    \"Àdáhùn\\s*:\",\n",
    "    \"Ànúgọ\\s*:\",\n",
    "    \"Àṣàyàn\\s*:\",\n",
    "    r'\"Answer\":\\s*',   # Added\n",
    "    r\"'Answer':\\s*\",   # Added\n",
    "    \"<Answer>\\s*\",     # Added\n",
    "]\n",
    "\n",
    "# Modify for \"A-D\" or 'A-D'\n",
    "MULTILINGUAL_ANSWER_PATTERN_TEMPLATE = (\n",
    "    r\"(?i){}[ \\t]*[\\\"\\']?([A-D]|[أ-د]|[অ]|[ব]|[ড]|[ঢ]|[Ａ]|[Ｂ]|[Ｃ]|[Ｄ])[\\\"\\']?\"\n",
    ")\n",
    "\n",
    "def normalize_extracted_answer(extracted_answer: str) -> str:\n",
    "    return (\n",
    "        # In arabic these are the letters used for A-D in multiple choice questions\n",
    "        extracted_answer.replace(\"أ\", \" A\")\n",
    "        .replace(\"ب\", \" B\")\n",
    "        .replace(\"ج\", \" C\")\n",
    "        .replace(\"د\", \" D\")\n",
    "        # In Bengali these are the letters used for A-D in multiple choice questions\n",
    "        .replace(\"অ\", \" A\")\n",
    "        .replace(\"ব\", \" B\")\n",
    "        .replace(\"ড\", \" C\")\n",
    "        .replace(\"ঢ\", \" D\")\n",
    "        # In Japanese these are the letters sometimes used for A-D in multiple choice questions\n",
    "        .replace(\"Ａ\", \" A\")\n",
    "        .replace(\"Ｂ\", \" B\")\n",
    "        .replace(\"Ｃ\", \" C\")\n",
    "        .replace(\"Ｄ\", \" D\")\n",
    "        .strip()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f1c1253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# From common.py\n",
    "\n",
    "\n",
    "def extract_answer_from_response(response_text: str) -> Optional[str]:\n",
    "    extracted_answer = None\n",
    "    for answer_regex in MULTILINGUAL_ANSWER_REGEXES:\n",
    "        regex = MULTILINGUAL_ANSWER_PATTERN_TEMPLATE.format(answer_regex)\n",
    "        match = re.search(regex, response_text)\n",
    "        if match:\n",
    "            extracted_answer = normalize_extracted_answer(match.group(1))\n",
    "            # print(extracted_answer)\n",
    "            break\n",
    "    return extracted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "1b37e316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "print(extract_answer_from_response(\"TEST A TEST\"))\n",
    "print(extract_answer_from_response(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4f69f8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
       " 'kwargs': {'content': \"Let's analyze each statement.\\nStatement 1: Every ideal in a ring is a subring of the ring.\\nFor a subset $I$ of a ring $R$ to be an ideal, it must satisfy:\\n1. $I$ is a subgroup of $R$ under addition.\\n2. For any $r \\\\in R$ and $x \\\\in I$, $rx \\\\in I$ and $xr \\\\in I$.\\nFor a subset $S$ of a ring $R$ to be a subring, it must satisfy:\\n1. $S$ is a subgroup of $R$ under addition.\\n2. $S$ is closed under multiplication.\\n\\nAn ideal $I$ must be a subgroup under addition, but it is not necessarily closed under multiplication. For example, consider the ring $\\\\mathbb{Z}$ and the ideal $2\\\\mathbb{Z}$. $2\\\\mathbb{Z}$ is an ideal of $\\\\mathbb{Z}$, but $2 \\\\in 2\\\\mathbb{Z}$ and $2 \\\\cdot 2 = 4 \\\\in 2\\\\mathbb{Z}$. If we consider the ideal $2\\\\mathbb{Z}$ in $\\\\mathbb{Z}$, $2\\\\mathbb{Z}$ is a subring of $\\\\mathbb{Z}$ as well. However, consider the ring $\\\\mathbb{Z}_6$ and the ideal $I = \\\\{0, 2, 4\\\\}$. $I$ is an ideal of $\\\\mathbb{Z}_6$ and is a subring of $\\\\mathbb{Z}_6$ because $2 \\\\cdot 2 = 4 \\\\in I$, $2 \\\\cdot 4 = 8 \\\\equiv 2 \\\\pmod{6} \\\\in I$, $4 \\\\cdot 4 = 16 \\\\equiv 4 \\\\pmod{6} \\\\in I$. However, if $I$ is an ideal, it is not always a subring. For $I$ to be a subring, it must contain the multiplicative identity.\\nFor example, consider the ring $\\\\mathbb{Z}$ and the ideal $2\\\\mathbb{Z}$. The multiplicative identity $1 \\\\notin 2\\\\mathbb{Z}$, so $2\\\\mathbb{Z}$ is not a subring.\\nThus, statement 1 is false.\\n\\nStatement 2: Every subring of every ring is an ideal of the ring.\\nThis statement is false. Consider the ring $\\\\mathbb{Z}$ and the subring $S = \\\\mathbb{Z}$. Let $R = \\\\mathbb{Z}$. $S = \\\\mathbb{Z}$ is a subring of $R = \\\\mathbb{Z}$.\\nLet $r \\\\in R$ and $x \\\\in S$. Then $r \\\\in \\\\mathbb{Z}$ and $x \\\\in \\\\mathbb{Z}$. Since $rx \\\\in \\\\mathbb{Z}$ and $xr \\\\in \\\\mathbb{Z}$, $rx \\\\in S$ and $xr \\\\in S$. So $\\\\mathbb{Z}$ is an ideal of $\\\\mathbb{Z}$. However, consider the ring $\\\\mathbb{Z}_6$ and the subring $S = \\\\{0, 1\\\\}$. $2 \\\\in \\\\mathbb{Z}_6$ and $1 \\\\in S$. $2 \\\\cdot 1 = 2 \\\\notin S$. Therefore, $S$ is not an ideal of $\\\\mathbb{Z}_6$. So statement 2 is false.\\n\\nAnswer: B\",\n",
       "  'response_metadata': {'prompt_feedback': {'block_reason': 0,\n",
       "    'safety_ratings': []},\n",
       "   'finish_reason': 'STOP',\n",
       "   'model_name': 'gemini-2.0-flash',\n",
       "   'safety_ratings': []},\n",
       "  'type': 'ai',\n",
       "  'id': 'run--597a2741-ac87-483e-a88e-58b2201ea514-0',\n",
       "  'usage_metadata': {'input_tokens': 103,\n",
       "   'output_tokens': 722,\n",
       "   'total_tokens': 825,\n",
       "   'input_token_details': {'cache_read': 0}},\n",
       "  'tool_calls': [],\n",
       "  'invalid_tool_calls': []}}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The experimental reuslt for saving?\n",
    "response.to_json()#['kwargs']\n",
    " #to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe19f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Draft\n",
    "experiment_save_dict = {\n",
    "    \"Model\": \"\",\n",
    "    \"Question id\": \"\",\n",
    "    \"Shuffle method\": \"\",\n",
    "    \"Original to shuffled\": \"\",\n",
    "    \"Input format\": \"\",\n",
    "    \"Output format\": \"\",\n",
    "    \"Query\": \"\",\n",
    "    \"Language\": \"\",\n",
    "    \"Subtask\": \"\",\n",
    "    \"Original correct answer\": \"\",\n",
    "    \"Shuffled correct answer\": \"\",\n",
    "    \"Response answer\": \"\",\n",
    "    \"Model output\": \"\",  # Output text only\n",
    "    \"Full response\": \"\", # All the output\n",
    "}\n",
    "\n",
    "experiment_list = [experiment_save_dict]\n",
    "experiment_df = pd.DataFrame(experiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a815c0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Question id</th>\n",
       "      <th>Shuffle method</th>\n",
       "      <th>Original to shuffled</th>\n",
       "      <th>Input format</th>\n",
       "      <th>Output format</th>\n",
       "      <th>Query</th>\n",
       "      <th>Language</th>\n",
       "      <th>Subtask</th>\n",
       "      <th>Original correct answer</th>\n",
       "      <th>Shuffled correct answer</th>\n",
       "      <th>Response answer</th>\n",
       "      <th>Model output</th>\n",
       "      <th>Full response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Question id Shuffle method Original to shuffled Input format  \\\n",
       "0                                                                      \n",
       "\n",
       "  Output format Query Language Subtask Original correct answer  \\\n",
       "0                                                                \n",
       "\n",
       "  Shuffled correct answer Response answer Model output Full response  \n",
       "0                                                                     "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
