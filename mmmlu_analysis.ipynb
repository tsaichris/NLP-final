{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97bbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmmlu_preparer\n",
    "from mmmlu_preparer.read_mmmlu_dataset import (\n",
    "    TARGET_SUBTASKS,\n",
    "    MMMLULanguage,\n",
    "    create_mmmlu_dataset,\n",
    "    sample_first_n_data_from_subtask\n",
    ")\n",
    "from mmmlu_preparer.query_formats import (\n",
    "    InputFormat,\n",
    "    OutputFormat,\n",
    "    ShuffleMethod,\n",
    "    get_current_queries\n",
    ")\n",
    "\n",
    "\n",
    "model_list = ['gemini-2.0-flash', 'mistral-small-2503', 'llama-v3p1-8b-instruct']\n",
    "lang_list = list(MMMLULanguage)\n",
    "input_output_list = [('base', 'base'), ('base', 'json-full'), ('base', 'xml-full'),('json', 'base'),('json', 'json-full'),('json', 'xml-full'), ('xml', 'base'),('xml', 'json-full'),('xml', 'xml-full')]\n",
    "shuffle_list = list(ShuffleMethod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Format\n",
    "\n",
    "# experiment_save_dict = {\n",
    "#     \"Model\": \"\",\n",
    "#     \"Language\": \"\",\n",
    "#     \"Subtask\": \"\",\n",
    "#     \"Question id in subtask\": \"\",\n",
    "#     \"Shuffle method\": \"\",\n",
    "#     \"Original to shuffled\": \"\",\n",
    "#     \"Input format\": \"\",\n",
    "#     \"Output format\": \"\",\n",
    "#     \"Query\": \"\",\n",
    "#     \"Original correct answer\": \"\",\n",
    "#     \"Shuffled correct answer\": \"\",\n",
    "#     \"Response answer\": \"\",\n",
    "#     \"Model output\": \"\",  # Output text only\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import mmmlu_preparer\n",
    "from mmmlu_preparer.answer_extract import extract_answer_from_response\n",
    "\n",
    "\n",
    "chosen_subtasks = TARGET_SUBTASKS\n",
    "combinations = list(product(model_list, input_output_list, shuffle_list))\n",
    "merged_result_dict = defaultdict(list) # \"model_name:\"\n",
    "\n",
    "generate_csv = True\n",
    "\n",
    "if generate_csv:\n",
    "    for lang_enum in lang_list:\n",
    "        mmmlu_ds = create_mmmlu_dataset(lang_enum)\n",
    "        chosen_subtasks = TARGET_SUBTASKS\n",
    "        mmmlu_subset = sample_first_n_data_from_subtask(mmmlu_ds, chosen_subtasks)\n",
    "\n",
    "        for curr_combo in tqdm(combinations):\n",
    "            model_name, format_tuple, shuffle_method_enum = curr_combo\n",
    "            if lang_enum is MMMLULanguage.EN and shuffle_method_enum in [ShuffleMethod.MOST_KANA_RATIO, ShuffleMethod.FEWEST_KANA_RATIO]:\n",
    "                continue\n",
    "            input_format, output_format = format_tuple\n",
    "            lang_string = lang_enum.value.lower().replace(\"_\", \"-\")\n",
    "            shuffle_method = shuffle_method_enum.name.lower().replace(\"_\", \"-\")\n",
    "            log_file_name = f\"{model_name}_{lang_string}_{input_format}_input_{output_format}_output_{shuffle_method}_shuffle.jsonl\"\n",
    "            print(log_file_name)\n",
    "\n",
    "            input_format_enum = InputFormat(input_format.replace(\"-\", \"_\"))\n",
    "            output_format_enum = OutputFormat[output_format.replace(\"-\", \"_\").upper()]\n",
    "\n",
    "            curr_queries = get_current_queries(mmmlu_subset,\n",
    "                                                lang_enum,\n",
    "                                                chosen_subtasks,\n",
    "                                                input_format_enum,\n",
    "                                                output_format_enum,\n",
    "                                                shuffle_method_enum,\n",
    "                                                )\n",
    "\n",
    "            log_path = Path(f\"./mmmlu_output/{log_file_name}\")\n",
    "            if log_path.exists():\n",
    "                with log_path.open('r', encoding='utf-8') as file:\n",
    "                    for idx, line in enumerate(file):\n",
    "                        curr_query = curr_queries[idx]\n",
    "                        curr_result_dict = {}\n",
    "                        response_dict = json.loads(line.strip())\n",
    "                        response = response_dict['kwargs']['content']\n",
    "                        extracted_answer = extract_answer_from_response(response)\n",
    "\n",
    "                        if extracted_answer is None:\n",
    "                            extracted_answer = 'Others'\n",
    "\n",
    "                        curr_result_dict['Model'] = model_name\n",
    "                        curr_result_dict['Language'] = lang_string\n",
    "                        curr_result_dict['Subtask'] = curr_query['Subtask']\n",
    "                        curr_result_dict['Question id in subtask'] = curr_query['Question id in subtask']\n",
    "                        curr_result_dict['Shuffle method'] = shuffle_method\n",
    "                        curr_result_dict['Original to shuffled'] = curr_query['Original to shuffled']\n",
    "                        curr_result_dict['Input format'] = input_format\n",
    "                        curr_result_dict['Output format'] = output_format\n",
    "                        curr_result_dict['Query'] = curr_query['Query']\n",
    "                        curr_result_dict['Original correct answer'] = curr_query['Original correct answer']\n",
    "                        curr_result_dict['Shuffled correct answer'] = curr_query['Shuffled correct answer']\n",
    "                        curr_result_dict['Response answer'] = extracted_answer\n",
    "                        curr_result_dict['Model output'] = response\n",
    "                        if model_name == 'llama-v3p1-8b-instruct':\n",
    "                            curr_result_dict['logprobs'] = response_dict['kwargs']['response_metadata']['logprobs']\n",
    "                        else:\n",
    "                            curr_result_dict['logprobs'] = []\n",
    "\n",
    "                        merged_result_dict[model_name].append(curr_result_dict)\n",
    "\n",
    "    experiment_dfs = [pd.DataFrame(val_list) for val_list in merged_result_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_csv:\n",
    "    for df in experiment_dfs:\n",
    "        model_name = df['Model'][0]\n",
    "        print(model_name)\n",
    "        csv_name = f\"{model_name}_merged_results.csv\"\n",
    "        df.to_csv(csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd492003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "target_model_name = model_list[0]\n",
    "test_df = pd.read_csv(f'{target_model_name}_merged_results.csv', engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def shuffle_map_type_conversion(shuffle_map):\n",
    "    \"\"\"Convert shuffled map string to python dictionary\"\"\"\n",
    "    if isinstance(shuffle_map, str):\n",
    "        return ast.literal_eval(shuffle_map)\n",
    "    return shuffle_map\n",
    "\n",
    "#test_df['Shuffled to Original']\n",
    "test_df['Original to shuffled'] = test_df['Original to shuffled'].apply(shuffle_map_type_conversion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f735a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Shuffled to Original']  = test_df['Original to shuffled'].apply(lambda x: {val:key for key, val in x.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f65a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_answer(row):\n",
    "    \"\"\"Convert model's answer to the id in the default order\"\"\"\n",
    "    response_ansewr = row['Response answer']\n",
    "    shuffled_to_original = row['Shuffled to Original']\n",
    "    if response_ansewr in shuffled_to_original:\n",
    "        return shuffled_to_original[response_ansewr]\n",
    "    return response_ansewr # nan\n",
    "\n",
    "test_df['Response answer id in default'] = test_df.apply(map_answer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "def append_new_level(levels: list[str], append_level: Optional[Union[str, list[str]]] = None) -> list[str]:\n",
    "    \"\"\"Add new levels for grouping\"\"\"\n",
    "    if append_level is not None:\n",
    "        if isinstance(append_level, str):\n",
    "            append_level = [append_level]\n",
    "        levels.extend(append_level)\n",
    "    return levels\n",
    "\n",
    "def get_metric_level_dict(\n",
    "    metric_df: pd.DataFrame,\n",
    "    target_key: str,\n",
    "    append_levels: Optional[Union[str, list[str]]] = None,\n",
    ") -> dict:\n",
    "    metric_dict = {}\n",
    "\n",
    "    # Language level\n",
    "    # Order Sensitivity across Languages\n",
    "    language_level = append_new_level(['Language'], append_levels)\n",
    "    metric_dict['Language'] = metric_df.groupby(language_level)[target_key].mean()\n",
    "\n",
    "    # Subtask level\n",
    "    # Order Sensitivity across subtasks\n",
    "    subtask_level = append_new_level(['Subtask'], append_levels)\n",
    "    metric_dict['Subtask'] = metric_df.groupby(subtask_level)[target_key].mean()\n",
    "\n",
    "    # Language & format level\n",
    "    # Order Sensitivity across Languages\n",
    "    language_format_level = append_new_level(['Language', \"Input format\", \"Output format\"], append_levels)\n",
    "    metric_dict['Language_format'] = metric_df.groupby(language_format_level)[target_key].mean()\n",
    "\n",
    "    # Subtask & Format level\n",
    "    # Does input/output formatting increase or reduce order bias?\n",
    "    subtask_format_level = append_new_level(['Subtask', \"Input format\", \"Output format\"], append_levels)\n",
    "    metric_dict['Subtask_format'] = metric_df.groupby(subtask_format_level)[target_key].mean()\n",
    "\n",
    "    # all level\n",
    "    all_level = append_new_level(['Language', 'Subtask', 'Input format', 'Output format'], append_levels)\n",
    "    metric_dict['All'] = metric_df.groupby(all_level)[target_key].mean()\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fr(base_df: pd.DataFrame, forward_method: str, backward_method: str) -> dict:\n",
    "    \"\"\"Compute Fluctuation Rate in different level (language, subtask, ...)\n",
    "    Ref: https://aclanthology.org/2024.findings-acl.333/\n",
    "    \"\"\"\n",
    "    forward_df = base_df[base_df['Shuffle method'] == forward_method].reset_index()\n",
    "    backward_df = base_df[base_df['Shuffle method'] == backward_method].reset_index()\n",
    "\n",
    "    # Map invalid answer to 'Others'\n",
    "    forward_df['Response answer id in default'] = forward_df['Response answer id in default'].fillna(\"Others\")\n",
    "    backward_df['Response answer id in default'] = backward_df['Response answer id in default'].fillna(\"Others\")\n",
    "\n",
    "    target_key = 'Forward != backward'\n",
    "    forward_df[target_key] = (forward_df['Response answer id in default'] != backward_df['Response answer id in default'])\n",
    "    return get_metric_level_dict(forward_df, target_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe14a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(base_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Compute accuracy in different level (language, subtask, ...)\"\"\"\n",
    "    base_df = base_df.copy()\n",
    "    target_key = 'Is correct response'\n",
    "    base_df[target_key] = base_df['Response answer'] == base_df['Shuffled correct answer']\n",
    "\n",
    "    append_levels = 'Shuffle method'\n",
    "    return get_metric_level_dict(base_df, target_key, append_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e321b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_option_acc_dict(base_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Compute option accuracy in different level (language, subtask, ...)\"\"\"\n",
    "    base_df = base_df.copy()\n",
    "    target_key = 'Is correct response'\n",
    "    base_df[target_key] = base_df['Response answer'] == base_df['Shuffled correct answer']\n",
    "\n",
    "    append_levels = ['Shuffle method', 'Original correct answer']\n",
    "    return get_metric_level_dict(base_df, target_key, append_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict = compute_accuracy(test_df)\n",
    "acc_dict['Subtask_format']#.loc[('ja-jp', 'base', 'base')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict['All']#.groupby([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca408bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "option_acc_dict = get_option_acc_dict(test_df)\n",
    "\n",
    "# Rstd\n",
    "all_rstd = option_acc_dict['All'].groupby(['Language', \"Subtask\", \"Input format\", \"Output format\", \"Shuffle method\"]).std(ddof=0)\n",
    "print(all_rstd)\n",
    "\n",
    "# RSD\n",
    "all_rsd = all_rstd / acc_dict[\"All\"]\n",
    "print(all_rsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748fd0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rstd\n",
    "lang_rstd = option_acc_dict['Language_format'].groupby(['Language', \"Input format\", \"Output format\", \"Shuffle method\"]).std(ddof=0)\n",
    "print(lang_rstd)\n",
    "\n",
    "# # RSD\n",
    "lang_rsd = lang_rstd / acc_dict[\"Language_format\"]\n",
    "print(lang_rsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2365517",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_rsd.xs(('default'), level='Shuffle method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f238def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rstd\n",
    "subtask_rstd = option_acc_dict['Subtask_format'].groupby(['Subtask', \"Input format\", \"Output format\", \"Shuffle method\"]).std(ddof=0)\n",
    "print(subtask_rstd)\n",
    "\n",
    "# # RSD\n",
    "subtask_rsd = subtask_rstd / acc_dict[\"Subtask_format\"]\n",
    "print(subtask_rsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "option_acc_dict['Language_format'].loc[('en', 'base', 'base')].groupby(\"Shuffle method\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e33c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_backward_pair = [('default', 'reverse'), ('longest-first', 'shortest-first'), ('most-kana-ratio', 'fewest-kana-ratio')]\n",
    "\n",
    "\n",
    "for forward_method, backward_method in forward_backward_pair:\n",
    "    fr_dict = compute_fr(test_df, forward_method, backward_method)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict['Language_format']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
