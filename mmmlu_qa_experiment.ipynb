{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc100dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import langchain\n",
    "\n",
    "#Model = \"gemini\"\n",
    "Model = \"llama\"\n",
    "\n",
    "# Langsmith\n",
    "langchain.debug = False\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "# if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "# if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "#     os.environ[\"LANGSMITH_PROJECT\"] = \"nlp_final\"\n",
    "if Model == 'gemini':\n",
    "    if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n",
    "    if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "else:\n",
    "    if \"FIREWORKS_API_KEY\" not in os.environ:\n",
    "        os.environ[\"FIREWORKS_API_KEY\"] = \"\"\n",
    "    if not os.environ.get(\"FIREWORKS_API_KEY\"):\n",
    "        os.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass(\"Enter API key for Fireworks: \")\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2727bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmmlu_preparer\n",
    "from mmmlu_preparer.read_mmmlu_dataset import (\n",
    "    TARGET_SUBTASKS,\n",
    "    MMMLULanguage,\n",
    "    create_mmmlu_dataset,\n",
    "    sample_first_n_data_from_subtask\n",
    ")\n",
    "\n",
    "lang_list = [\"EN\", \"JA_JP\"]\n",
    "curr_language = lang_list[0]\n",
    "dataset_language_enum = MMMLULanguage[curr_language]\n",
    "\n",
    "mmmlu_ds = create_mmmlu_dataset(dataset_language_enum)\n",
    "chosen_subtasks = TARGET_SUBTASKS\n",
    "mmmlu_subset = sample_first_n_data_from_subtask(mmmlu_ds, chosen_subtasks)\n",
    "mmmlu_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmmlu_subset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc628684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmmlu_preparer.answer_extract import extract_answer_from_response\n",
    "print(extract_answer_from_response(\"TEST A TEST\"))\n",
    "print(extract_answer_from_response(\"Answer: B\"))\n",
    "print(extract_answer_from_response(\"<Answer> D\"))\n",
    "print(extract_answer_from_response(\"'Answer': C\"))\n",
    "print(extract_answer_from_response('\"Answer\": A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Draft\n",
    "experiment_save_dict = {\n",
    "    \"Model\": \"\",\n",
    "    \"Question id\": \"\",\n",
    "    \"Shuffle method\": \"\",\n",
    "    \"Original to shuffled\": \"\",\n",
    "    \"Input format\": \"\",\n",
    "    \"Output format\": \"\",\n",
    "    \"Query\": \"\",\n",
    "    \"Language\": \"\",\n",
    "    \"Subtask\": \"\",\n",
    "    \"Original correct answer\": \"\",\n",
    "    \"Shuffled correct answer\": \"\",\n",
    "    \"Response answer\": \"\",\n",
    "    \"Model output\": \"\",  # Output text only\n",
    "    \"Full response\": \"\", # All the output\n",
    "}\n",
    "\n",
    "experiment_list = [experiment_save_dict]\n",
    "experiment_df = pd.DataFrame(experiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb73003",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a76ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir = \"mmmlu_output\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# llama3 rate: 6000 / 100 qps\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=40,\n",
    "    check_every_n_seconds=0.1,\n",
    "    max_bucket_size=40,\n",
    ")\n",
    "\n",
    "if Model == \"gemini\":\n",
    "    model_name = \"gemini-2.0-flash\"\n",
    "    model = init_chat_model(model_name,\n",
    "                        model_provider=\"google_genai\",\n",
    "                        rate_limiter=rate_limiter,\n",
    "                        temperature=0.0,\n",
    "                        max_tokens=4096\n",
    "                        )\n",
    "else:\n",
    "    #model_name = \"llama-v3p1-405b-instruct\"\n",
    "    model_name = \"llama-v3p1-8b-instruct\"\n",
    "\n",
    "    model = ChatFireworks(\n",
    "        model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\",\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        logprobs = 5,\n",
    "        rate_limiter=rate_limiter\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "# text: The text to translate\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4286549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmmlu_preparer.query_formats import (\n",
    "    get_current_queries,\n",
    "    InputFormat,\n",
    "    OutputFormat,\n",
    "    ShuffleMethod\n",
    ")\n",
    "\n",
    "# BASE, JSON, XML\n",
    "curr_input_format = InputFormat.XML\n",
    "\n",
    "# BASE, JSON_FULL, XML_FULL\n",
    "curr_output_format = OutputFormat.XML_FULL\n",
    "\n",
    "# DEFAULT, REVERSE, LONGEST_FIRST, SHORTEST_FIRST, MOST_KANA_RATIO, FEWEST_KANA_RATIO\n",
    "curr_shuffle_method = ShuffleMethod.REVERSE\n",
    "\n",
    "input_format_save_name = curr_input_format.value.lower().replace(\"_\", \"-\")\n",
    "output_format_save_name = curr_output_format.name.lower().replace(\"_\", \"-\")\n",
    "shuffle_method_save_name = curr_shuffle_method.name.lower().replace(\"_\", \"-\")\n",
    "\n",
    "language_name = curr_language.lower().replace(\"_\", \"-\")\n",
    "\n",
    "save_name = f\"{model_name}_{language_name}_{input_format_save_name}_input_{output_format_save_name}_output_{shuffle_method_save_name}_shuffle\"\n",
    "save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e656026",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(curr_output_format.name.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87522be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_queries = get_current_queries(mmmlu_subset,\n",
    "                                   dataset_language_enum,\n",
    "                                   chosen_subtasks,\n",
    "                                   curr_input_format,\n",
    "                                   curr_output_format,\n",
    "                                   curr_shuffle_method,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmmlu_subset.filter(lambda x: x['Subject'] == \"abstract_algebra\")[0]['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d931d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(curr_queries[1000]['Query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf258a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import trange\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from mmmlu_preparer.logprobs import extract_answer_logprobs\n",
    "import asyncio\n",
    "\n",
    "async def retry_bad_response(bad_prompt, model, curr_output_format, max_retries=5):\n",
    "    \"\"\"Retry a single bad prompt until we get a good response\"\"\"\n",
    "    \n",
    "    retry_instructions = [\n",
    "        \"\\n\\nSkip the reasoning steps, give the answer directly.\",\n",
    "        \"\\n\\nProvide only the final answer without explanation.\", \n",
    "        \"\\n\\nBe concise and direct in your response.\",\n",
    "        \"\\n\\nAnswer briefly without showing work.\"\n",
    "    ]\n",
    "    \n",
    "    original_text = bad_prompt.messages[0].content\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Create modified text with retry instruction\n",
    "            if attempt < len(retry_instructions):\n",
    "                modified_text = original_text + retry_instructions[attempt]\n",
    "            else:\n",
    "                modified_text = original_text  # Fallback to original\n",
    "            \n",
    "            # Create new prompt using the template\n",
    "            modified_text_query = {\"text\": modified_text}\n",
    "            modified_prompt = prompt_template.batch([modified_text_query])[0]\n",
    "            \n",
    "            response = await model.abatch([modified_prompt])\n",
    "            response_dict = response[0].to_json()\n",
    "            \n",
    "            # Check if this retry is good (< 4000 tokens)\n",
    "            completion_tokens = response_dict['kwargs']['response_metadata']['token_usage']['completion_tokens']\n",
    "            if completion_tokens <= 4000:\n",
    "                # Add processed logprobs for good response\n",
    "                #print(response_dict)\n",
    "                \n",
    "                \n",
    "                return response[0], response_dict\n",
    "            else:\n",
    "                print(f\"Retry attempt {attempt + 1} still bad ({completion_tokens} tokens)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in retry attempt {attempt + 1}: {e}\")\n",
    "            \n",
    "        # Add a small delay between retries\n",
    "        await asyncio.sleep(2)\n",
    "    \n",
    "    # If all retries failed, return None\n",
    "    print(f\"All {max_retries} retry attempts failed\")\n",
    "    return None, None\n",
    "\n",
    "async def run_experiemnts(queries: list[dict], save_path: str, try_first_n: Optional[int] = None, curr_output_format = None) -> list[dict]:\n",
    "    text_queries = [{\"text\": query['Query']} for query in queries]\n",
    "    \n",
    "    input_prompts = prompt_template.batch(text_queries)\n",
    "\n",
    "    results = []\n",
    "    mini_batch_size = 20\n",
    "\n",
    "    target_save_path = Path(save_path)\n",
    "    if target_save_path.suffix != \".jsonl\":\n",
    "        print(\"Output should be jsonl file\")\n",
    "        target_save_path = target_save_path.with_suffix(\".jsonl\")\n",
    "    target_save_path.touch()\n",
    "\n",
    "    with target_save_path.open('r', encoding='utf-8') as file:\n",
    "        # Count the nubmer of lines\n",
    "        start_idx = sum(1 for line in file if line.strip())\n",
    "    print(f\"Start from {start_idx = }\")\n",
    "\n",
    "    total_process_num = len(text_queries)\n",
    "    if try_first_n is not None:\n",
    "        total_process_num = start_idx + try_first_n\n",
    "\n",
    "    for batch_i in trange(start_idx, total_process_num, mini_batch_size):\n",
    "        \n",
    "        try:\n",
    "\n",
    "            batched_prompts = input_prompts[batch_i:batch_i + mini_batch_size]\n",
    "            responses = await model.abatch(batched_prompts)\n",
    "            #print(responses)\n",
    "            results.extend(responses)\n",
    "            with target_save_path.open('a', encoding='utf-8') as file:\n",
    "                for i, response in enumerate(responses):\n",
    "                    retry_count = 0\n",
    "                    \n",
    "                    if Model == \"gemini\":\n",
    "                        json.dump(response.to_json(), file, ensure_ascii=False)\n",
    "                        file.write(\"\\n\")\n",
    "                    else:\n",
    "                        response_dict = response.to_json()\n",
    "                        # if the model keep repeating the same answer\n",
    "                        completion_tokens = response_dict['kwargs']['response_metadata']['token_usage']['completion_tokens']\n",
    "                            \n",
    "                        # Check if response is bad (> 4000 tokens)\n",
    "                        if completion_tokens > 4000:\n",
    "                            print(f\"Bad response detected at index {i} ({completion_tokens} tokens), retrying...\")\n",
    "                            retry_count += 1\n",
    "                            \n",
    "                            # Get the original prompt for this response\n",
    "                            bad_prompt = batched_prompts[i]\n",
    "                            \n",
    "                            # Retry until we get a good response\n",
    "                            response, response_dict = await retry_bad_response(\n",
    "                                bad_prompt, model, curr_output_format\n",
    "                            )\n",
    "                            \n",
    "                            if response is not None:\n",
    "                                print(f\"Successfully retried response {i}\")\n",
    "                            else:\n",
    "                                # If all retries failed\n",
    "                                print(f\"use bad response\")\n",
    "                                continue\n",
    "                        \n",
    "                        # add processed logprobs \n",
    "                        answer_probs = extract_answer_logprobs(response, curr_output_format)\n",
    "                        response_dict['kwargs']['response_metadata']['logprobs'] = answer_probs\n",
    "                        \n",
    "                        json.dump(response_dict, file, ensure_ascii=False)\n",
    "                        file.write(\"\\n\")\n",
    "                file.flush()\n",
    "            print(f\"Finish {batch_i + mini_batch_size} data\")\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Rate limit break\n",
    "            print(f\"Current idx:{batch_i}\\nencounters exception: {e}\\nIt might be daily rate limit or error.\")\n",
    "            break\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"{save_dir}/{save_name}.jsonl\"\n",
    "results = await run_experiemnts(curr_queries, save_path, try_first_n=None, curr_output_format = curr_output_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmmlu_preparer.query_formats import (\n",
    "    get_current_queries,\n",
    "    InputFormat,\n",
    "    OutputFormat,\n",
    "    ShuffleMethod\n",
    ")\n",
    "for method in ['LONGEST_FIRST', 'SHORTEST_FIRST']:\n",
    "    for input_format in ['BASE', 'JSON', 'XML']:\n",
    "        for output_format in ['BASE', 'JSON_FULL', 'XML_FULL']:\n",
    "            \n",
    "            # BASE, JSON, XML\n",
    "            if input_format == 'BASE':\n",
    "                curr_input_format = InputFormat.BASE\n",
    "            elif input_format == 'JSON':\n",
    "                curr_input_format = InputFormat.JSON\n",
    "            elif input_format == 'XML':\n",
    "                curr_input_format = InputFormat.XML\n",
    "\n",
    "            if output_format == 'BASE':\n",
    "                # BASE, JSON_FULL, XML_FULL\n",
    "                curr_output_format = OutputFormat.BASE\n",
    "            elif output_format == 'JSON_FULL':  \n",
    "                curr_output_format = OutputFormat.JSON_FULL\n",
    "            elif output_format == 'XML_FULL':\n",
    "                curr_output_format = OutputFormat.XML_FULL\n",
    "\n",
    "            if method == 'LONGEST_FIRST':\n",
    "                # DEFAULT, REVERSE, LONGEST_FIRST, SHORTEST_FIRST, MOST_KANA_RATIO, FEWEST_KANA_RATIO\n",
    "                curr_shuffle_method = ShuffleMethod.LONGEST_FIRST\n",
    "            elif method == 'SHORTEST_FIRST':\n",
    "                # DEFAULT, REVERSE, LONGEST_FIRST, SHORTEST_FIRST, MOST_KANA_RATIO, FEWEST_KANA_RATIO\n",
    "                curr_shuffle_method = ShuffleMethod.SHORTEST_FIRST\n",
    "\n",
    "\n",
    "                input_format_save_name = curr_input_format.value.lower().replace(\"_\", \"-\")\n",
    "                output_format_save_name = curr_output_format.name.lower().replace(\"_\", \"-\")\n",
    "                shuffle_method_save_name = curr_shuffle_method.name.lower().replace(\"_\", \"-\")\n",
    "\n",
    "                language_name = curr_language.lower().replace(\"_\", \"-\")\n",
    "\n",
    "                save_name = f\"{model_name}_{language_name}_{input_format_save_name}_input_{output_format_save_name}_output_{shuffle_method_save_name}_shuffle\"\n",
    "                save_name\n",
    "\n",
    "                curr_queries = get_current_queries(mmmlu_subset,\n",
    "                                                dataset_language_enum,\n",
    "                                                chosen_subtasks,\n",
    "                                                curr_input_format,\n",
    "                                                curr_output_format,\n",
    "                                                curr_shuffle_method,\n",
    "                                                )\n",
    "\n",
    "\n",
    "                save_path = f\"{save_dir}/{save_name}.jsonl\"\n",
    "                results = await run_experiemnts(curr_queries, save_path, try_first_n=None, curr_output_format = curr_output_format)\n",
    "                await asyncio.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "save_path = f\"{save_dir}/{save_name}.jsonl\"\n",
    "target_save_path = Path(save_path)\n",
    "with target_save_path.open('r', encoding='utf-8') as file:\n",
    "    result_dicts = [json.loads(line) for line in file if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ac99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_dicts[1]['kwargs']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1144e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = [result['kwargs']['content'] for result in result_dicts]\n",
    "output_answer = []\n",
    "none_answer_indice = []\n",
    "none_answer_output = []\n",
    "for idx, output in enumerate(output_text):\n",
    "    extracted_answer = extract_answer_from_response(output)\n",
    "    output_answer.append(extracted_answer)\n",
    "    if extracted_answer is None:\n",
    "        none_answer_indice.append(idx)\n",
    "        none_answer_output.append(output)\n",
    "        #print(idx)\n",
    "        print(f\"{idx}:\\n{output = }\\n\")\n",
    "\n",
    "print(f\"{len(none_answer_indice) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "output_tokens_list = [result['kwargs']['usage_metadata']['total_tokens'] for result in result_dicts]\n",
    "print(np.argsort(-np.array(output_tokens_list)).tolist())\n",
    "print(np.sort(-np.array(output_tokens_list)).tolist())\n",
    "print(f\"median: {np.median(output_tokens_list)}\")\n",
    "print(f\"mean: {np.mean(output_tokens_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_dicts[7]['kwargs']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.output_parsers.json import JsonOutputParser\n",
    "# import re\n",
    "# from tqdm.auto import tqdm\n",
    "# parser = JsonOutputParser()\n",
    "\n",
    "# for result in tqdm(result_dicts):\n",
    "#     #try:\n",
    "#     string = result['kwargs']['content']\n",
    "\n",
    "#     def escape_single_backslash(match):\n",
    "#         c = match.group(0)\n",
    "#         return c.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "\n",
    "#     # ChatGPT\n",
    "#     string = re.sub(r'(?<!\\\\)\\\\(?![\\\\ntbrf\"u])', escape_single_backslash, string)\n",
    "\n",
    "#     x = (parser.parse(string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
