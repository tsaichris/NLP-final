{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc100dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import langchain\n",
    "\n",
    "#Model = \"gemini\"\n",
    "Model = \"llama\"\n",
    "\n",
    "# Langsmith\n",
    "langchain.debug = False\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "\n",
    "if Model == 'gemini':\n",
    "    if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n",
    "    if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "else:\n",
    "    if \"FIREWORKS_API_KEY\" not in os.environ:\n",
    "        os.environ[\"FIREWORKS_API_KEY\"] = \"\"\n",
    "    if not os.environ.get(\"FIREWORKS_API_KEY\"):\n",
    "        os.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass(\"Enter API key for Fireworks: \")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2727bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/home/TsaiChris/.conda/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Question id in subtask', 'Question', 'Subject', 'Answer', 'A', 'B', 'C', 'D'],\n",
       "    num_rows: 1700\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mmmlu_preparer\n",
    "from mmmlu_preparer.read_mmmlu_dataset import (\n",
    "    TARGET_SUBTASKS,\n",
    "    MMMLULanguage,\n",
    "    create_mmmlu_dataset,\n",
    "    sample_first_n_data_from_subtask\n",
    ")\n",
    "\n",
    "lang_list = [\"EN\", \"JA_JP\"]\n",
    "curr_language = lang_list[0]\n",
    "dataset_language_enum = MMMLULanguage[curr_language]\n",
    "\n",
    "mmmlu_ds = create_mmmlu_dataset(dataset_language_enum)\n",
    "chosen_subtasks = TARGET_SUBTASKS\n",
    "mmmlu_subset = sample_first_n_data_from_subtask(mmmlu_ds, chosen_subtasks)\n",
    "mmmlu_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf6f661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question id in subtask': 1,\n",
       " 'Question': 'Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the index of <p> in S_5.',\n",
       " 'Subject': 'abstract_algebra',\n",
       " 'Answer': 'C',\n",
       " 'A': '8',\n",
       " 'B': '2',\n",
       " 'C': '24',\n",
       " 'D': '120'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmmlu_subset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc628684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "B\n",
      "D\n",
      "C\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "from mmmlu_preparer.answer_extract import extract_answer_from_response\n",
    "print(extract_answer_from_response(\"TEST A TEST\"))\n",
    "print(extract_answer_from_response(\"Answer: B\"))\n",
    "print(extract_answer_from_response(\"<Answer> D\"))\n",
    "print(extract_answer_from_response(\"'Answer': C\"))\n",
    "print(extract_answer_from_response('\"Answer\": A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd3709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Draft\n",
    "experiment_save_dict = {\n",
    "    \"Model\": \"\",\n",
    "    \"Question id\": \"\",\n",
    "    \"Shuffle method\": \"\",\n",
    "    \"Original to shuffled\": \"\",\n",
    "    \"Input format\": \"\",\n",
    "    \"Output format\": \"\",\n",
    "    \"Query\": \"\",\n",
    "    \"Language\": \"\",\n",
    "    \"Subtask\": \"\",\n",
    "    \"Original correct answer\": \"\",\n",
    "    \"Shuffled correct answer\": \"\",\n",
    "    \"Response answer\": \"\",\n",
    "    \"Model output\": \"\",  # Output text only\n",
    "    \"Full response\": \"\", # All the output\n",
    "}\n",
    "\n",
    "experiment_list = [experiment_save_dict]\n",
    "experiment_df = pd.DataFrame(experiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb73003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Question id</th>\n",
       "      <th>Shuffle method</th>\n",
       "      <th>Original to shuffled</th>\n",
       "      <th>Input format</th>\n",
       "      <th>Output format</th>\n",
       "      <th>Query</th>\n",
       "      <th>Language</th>\n",
       "      <th>Subtask</th>\n",
       "      <th>Original correct answer</th>\n",
       "      <th>Shuffled correct answer</th>\n",
       "      <th>Response answer</th>\n",
       "      <th>Model output</th>\n",
       "      <th>Full response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Question id Shuffle method Original to shuffled Input format  \\\n",
       "0                                                                      \n",
       "\n",
       "  Output format Query Language Subtask Original correct answer  \\\n",
       "0                                                                \n",
       "\n",
       "  Shuffled correct answer Response answer Model output Full response  \n",
       "0                                                                     "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a76ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir = \"mmmlu_output\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce5a272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/home/TsaiChris/.conda/envs/nlp/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3490: UserWarning: WARNING! logprobs is not default parameter.\n",
      "                logprobs was transferred to model_kwargs.\n",
      "                Please confirm that logprobs is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# llama3 rate: 6000 / 100 qps\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=80,\n",
    "    check_every_n_seconds=0.1,\n",
    "    max_bucket_size=80,\n",
    ")\n",
    "\n",
    "if Model == \"gemini\":\n",
    "    model_name = \"gemini-2.0-flash\"\n",
    "    model = init_chat_model(model_name,\n",
    "                        model_provider=\"google_genai\",\n",
    "                        rate_limiter=rate_limiter,\n",
    "                        temperature=0.0,\n",
    "                        max_tokens=4096\n",
    "                        )\n",
    "else:\n",
    "    #model_name = \"llama-v3p1-405b-instruct\"\n",
    "    model_name = \"llama-v3p1-8b-instruct\"\n",
    "\n",
    "    model = ChatFireworks(\n",
    "        model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\",\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        logprobs = 5,\n",
    "        rate_limiter=rate_limiter\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "# text: The text to translate\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4286549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama-v3p1-8b-instruct_en_json_input_base_output_shortest-first_shuffle'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mmmlu_preparer.query_formats import (\n",
    "    get_current_queries,\n",
    "    InputFormat,\n",
    "    OutputFormat,\n",
    "    ShuffleMethod\n",
    ")\n",
    "\n",
    "# BASE, JSON, XML\n",
    "curr_input_format = InputFormat.JSON\n",
    "\n",
    "# BASE, JSON_FULL, XML_FULL\n",
    "curr_output_format = OutputFormat.BASE\n",
    "\n",
    "# DEFAULT, REVERSE, LONGEST_FIRST, SHORTEST_FIRST, MOST_KANA_RATIO, FEWEST_KANA_RATIO\n",
    "curr_shuffle_method = ShuffleMethod.SHORTEST_FIRST\n",
    "\n",
    "input_format_save_name = curr_input_format.value.lower().replace(\"_\", \"-\")\n",
    "output_format_save_name = curr_output_format.name.lower().replace(\"_\", \"-\")\n",
    "shuffle_method_save_name = curr_shuffle_method.name.lower().replace(\"_\", \"-\")\n",
    "\n",
    "language_name = curr_language.lower().replace(\"_\", \"-\")\n",
    "\n",
    "save_name = f\"{model_name}_{language_name}_{input_format_save_name}_input_{output_format_save_name}_output_{shuffle_method_save_name}_shuffle\"\n",
    "save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e656026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n"
     ]
    }
   ],
   "source": [
    "print(curr_output_format.name.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87522be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_queries = get_current_queries(mmmlu_subset,\n",
    "                                   dataset_language_enum,\n",
    "                                   chosen_subtasks,\n",
    "                                   curr_input_format,\n",
    "                                   curr_output_format,\n",
    "                                   curr_shuffle_method,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d6a03ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmmlu_subset.filter(lambda x: x['Subject'] == \"abstract_algebra\")[0]['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85d931d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Task\": \"Answer the following multiple choice question.\",\n",
      "    \"Output_format\": \"The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD.\",\n",
      "    \"Instruction\": \"Think step by step before answering.\",\n",
      "    \"Question\": \"As of 2016, about what percentage of adults aged 18 years or older were overweight?\",\n",
      "    \"Options\": {\n",
      "        \"A\": \"80%\",\n",
      "        \"B\": \"40%\",\n",
      "        \"C\": \"20%\",\n",
      "        \"D\": \"10%\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(curr_queries[1000]['Query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf258a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import trange\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from mmmlu_preparer.logprobs import extract_answer_logprobs\n",
    "import asyncio\n",
    "\n",
    "async def retry_bad_response(bad_prompt, model, curr_output_format, max_retries=5):\n",
    "    \"\"\"Retry a single bad prompt until we get a good response\"\"\"\n",
    "    \n",
    "    retry_instructions = [\n",
    "        \"\\n\\nSkip the reasoning steps, give the answer directly.\",\n",
    "        \"\\n\\nProvide only the final answer without explanation.\", \n",
    "        \"\\n\\nBe concise and direct in your response.\",\n",
    "        \"\\n\\nAnswer briefly without showing work.\"\n",
    "    ]\n",
    "    \n",
    "    original_text = bad_prompt.messages[0].content\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Create modified text with retry instruction\n",
    "            if attempt < len(retry_instructions):\n",
    "                modified_text = original_text + retry_instructions[attempt]\n",
    "            else:\n",
    "                modified_text = original_text  # Fallback to original\n",
    "            \n",
    "            # Create new prompt using the template\n",
    "            modified_text_query = {\"text\": modified_text}\n",
    "            modified_prompt = prompt_template.batch([modified_text_query])[0]\n",
    "            \n",
    "            response = await model.abatch([modified_prompt])\n",
    "            response_dict = response[0].to_json()\n",
    "            \n",
    "            # Check if this retry is good (< 4000 tokens)\n",
    "            completion_tokens = response_dict['kwargs']['response_metadata']['token_usage']['completion_tokens']\n",
    "            if completion_tokens <= 4000:\n",
    "                # Add processed logprobs for good response\n",
    "                #print(response_dict)\n",
    "                \n",
    "                \n",
    "                return response[0], response_dict\n",
    "            else:\n",
    "                print(f\"Retry attempt {attempt + 1} still bad ({completion_tokens} tokens)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in retry attempt {attempt + 1}: {e}\")\n",
    "            \n",
    "        # Add a small delay between retries\n",
    "        await asyncio.sleep(2)\n",
    "    \n",
    "    # If all retries failed, return None\n",
    "    print(f\"All {max_retries} retry attempts failed\")\n",
    "    return None, None\n",
    "\n",
    "async def run_experiemnts(queries: list[dict], save_path: str, try_first_n: Optional[int] = None, curr_output_format = None) -> list[dict]:\n",
    "    text_queries = [{\"text\": query['Query']} for query in queries]\n",
    "    \n",
    "    input_prompts = prompt_template.batch(text_queries)\n",
    "\n",
    "    results = []\n",
    "    mini_batch_size = 80\n",
    "\n",
    "    target_save_path = Path(save_path)\n",
    "    if target_save_path.suffix != \".jsonl\":\n",
    "        print(\"Output should be jsonl file\")\n",
    "        target_save_path = target_save_path.with_suffix(\".jsonl\")\n",
    "    target_save_path.touch()\n",
    "\n",
    "    with target_save_path.open('r', encoding='utf-8') as file:\n",
    "        # Count the nubmer of lines\n",
    "        start_idx = sum(1 for line in file if line.strip())\n",
    "    print(f\"Start from {start_idx = }\")\n",
    "\n",
    "    total_process_num = len(text_queries)\n",
    "    if try_first_n is not None:\n",
    "        total_process_num = start_idx + try_first_n\n",
    "\n",
    "    for batch_i in trange(start_idx, total_process_num, mini_batch_size):\n",
    "        \n",
    "        try:\n",
    "\n",
    "            batched_prompts = input_prompts[batch_i:batch_i + mini_batch_size]\n",
    "            responses = await model.abatch(batched_prompts)\n",
    "            #print(responses)\n",
    "            results.extend(responses)\n",
    "            with target_save_path.open('a', encoding='utf-8') as file:\n",
    "                for i, response in enumerate(responses):\n",
    "                    retry_count = 0\n",
    "                    \n",
    "                    if Model == \"gemini\":\n",
    "                        json.dump(response.to_json(), file, ensure_ascii=False)\n",
    "                        file.write(\"\\n\")\n",
    "                    else:\n",
    "                        response_dict = response.to_json()\n",
    "                        # if the model keep repeating the same answer\n",
    "                        completion_tokens = response_dict['kwargs']['response_metadata']['token_usage']['completion_tokens']\n",
    "                            \n",
    "                        # Check if response is bad (> 4000 tokens)\n",
    "                        if completion_tokens > 4000:\n",
    "                            print(f\"Bad response detected at index {i} ({completion_tokens} tokens), retrying...\")\n",
    "                            retry_count += 1\n",
    "                            \n",
    "                            # Get the original prompt for this response\n",
    "                            bad_prompt = batched_prompts[i]\n",
    "                            \n",
    "                            # Retry until we get a good response\n",
    "                            response, response_dict = await retry_bad_response(\n",
    "                                bad_prompt, model, curr_output_format\n",
    "                            )\n",
    "                            \n",
    "                            if response is not None:\n",
    "                                print(f\"Successfully retried response {i}\")\n",
    "                            else:\n",
    "                                # If all retries failed\n",
    "                                print(f\"use bad response\")\n",
    "                                continue\n",
    "                        \n",
    "                        # add processed logprobs \n",
    "                        answer_probs = extract_answer_logprobs(response, curr_output_format)\n",
    "                        response_dict['kwargs']['response_metadata']['logprobs'] = answer_probs\n",
    "                        \n",
    "                        json.dump(response_dict, file, ensure_ascii=False)\n",
    "                        file.write(\"\\n\")\n",
    "                file.flush()\n",
    "            print(f\"Finish {batch_i + mini_batch_size} data\")\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Rate limit break\n",
    "            print(f\"Current idx:{batch_i}\\nencounters exception: {e}\\nIt might be daily rate limit or error.\")\n",
    "            break\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7afb7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from start_idx = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad response detected at index 9 (4096 tokens), retrying...\n",
      "Successfully retried response 9\n",
      "Bad response detected at index 13 (4096 tokens), retrying...\n",
      "Successfully retried response 13\n",
      "Bad response detected at index 14 (4096 tokens), retrying...\n",
      "Successfully retried response 14\n",
      "Bad response detected at index 17 (4096 tokens), retrying...\n",
      "Successfully retried response 17\n",
      "Bad response detected at index 36 (4096 tokens), retrying...\n",
      "Successfully retried response 36\n",
      "Bad response detected at index 38 (4096 tokens), retrying...\n",
      "Successfully retried response 38\n",
      "Format-specific parsing failed for BASE, trying fallback methods...\n",
      "🔍 Starting FIXED XML answer search...\n",
      "📍 Found 'Answer' token at position 358\n",
      "📋 Context: prev='.\n",
      "\n",
      "' | current='Answer' | next=':'\n",
      "❌ Not XML pattern - prev doesn't end with '<' or next doesn't start with '>'\n",
      "❌ No XML answer pattern found\n",
      "All format-specific methods failed, using universal answer search...\n",
      "Found answer keyword at position 358: 'Answer'\n",
      "Found embedded answer letter 'A' in keyword token 'Answer' at position 358\n",
      "Bad response detected at index 70 (4096 tokens), retrying...\n",
      "Successfully retried response 70\n",
      "Bad response detected at index 78 (4096 tokens), retrying...\n",
      "Successfully retried response 78\n",
      "Finish 80 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1/22 [00:21<07:39, 21.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad response detected at index 18 (4096 tokens), retrying...\n",
      "Successfully retried response 18\n",
      "Finish 160 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2/22 [00:39<06:25, 19.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad response detected at index 45 (4096 tokens), retrying...\n",
      "Successfully retried response 45\n",
      "Finish 240 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3/22 [00:56<05:46, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 320 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4/22 [01:04<04:18, 14.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 400 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5/22 [01:09<03:03, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 480 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6/22 [01:14<02:20,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad response detected at index 31 (4096 tokens), retrying...\n",
      "Successfully retried response 31\n",
      "Bad response detected at index 42 (4096 tokens), retrying...\n",
      "Successfully retried response 42\n",
      "Bad response detected at index 49 (4096 tokens), retrying...\n",
      "Successfully retried response 49\n",
      "Bad response detected at index 72 (4096 tokens), retrying...\n",
      "Successfully retried response 72\n",
      "Bad response detected at index 74 (4096 tokens), retrying...\n",
      "Successfully retried response 74\n",
      "Finish 560 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7/22 [01:34<03:09, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad response detected at index 2 (4096 tokens), retrying...\n",
      "Successfully retried response 2\n",
      "Bad response detected at index 15 (4096 tokens), retrying...\n",
      "Successfully retried response 15\n",
      "Bad response detected at index 25 (4096 tokens), retrying...\n",
      "Successfully retried response 25\n",
      "Bad response detected at index 29 (4096 tokens), retrying...\n",
      "Successfully retried response 29\n",
      "Bad response detected at index 38 (4096 tokens), retrying...\n",
      "Successfully retried response 38\n",
      "Finish 640 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 8/22 [01:54<03:31, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad response detected at index 6 (4096 tokens), retrying...\n",
      "Successfully retried response 6\n",
      "Bad response detected at index 33 (4096 tokens), retrying...\n",
      "Successfully retried response 33\n",
      "Format-specific parsing failed for BASE, trying fallback methods...\n",
      "🔍 Starting FIXED XML answer search...\n",
      "📍 Found 'Answer' token at position 345\n",
      "📋 Context: prev='**' | current='Answer' | next=':**'\n",
      "❌ Not XML pattern - prev doesn't end with '<' or next doesn't start with '>'\n",
      "❌ No XML answer pattern found\n",
      "All format-specific methods failed, using universal answer search...\n",
      "Found answer keyword at position 1: ' answer'\n",
      "Found answer keyword at position 345: 'Answer'\n",
      "Found embedded answer letter 'A' in keyword token 'Answer' at position 345\n",
      "Format-specific parsing failed for BASE, trying fallback methods...\n",
      "🔍 Starting FIXED XML answer search...\n",
      "❌ No XML answer pattern found\n",
      "All format-specific methods failed, using universal answer search...\n",
      "Found answer keyword at position 1: ' answer'\n",
      "Found answer keyword at position 468: ' answer'\n",
      "Found answer letter 'B' at position 471\n",
      "Bad response detected at index 79 (4096 tokens), retrying...\n",
      "Successfully retried response 79\n",
      "Finish 720 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9/22 [02:13<03:30, 16.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format-specific parsing failed for BASE, trying fallback methods...\n",
      "🔍 Starting FIXED XML answer search...\n",
      "❌ No XML answer pattern found\n",
      "All format-specific methods failed, using universal answer search...\n",
      "Found answer keyword at position 357: ' answer'\n",
      "Found answer letter 'C' at position 360\n",
      "Bad response detected at index 26 (4096 tokens), retrying...\n",
      "Successfully retried response 26\n",
      "Format-specific parsing failed for BASE, trying fallback methods...\n",
      "🔍 Starting FIXED XML answer search...\n",
      "📍 Found 'Answer' token at position 597\n",
      "📋 Context: prev='.\n",
      "\n",
      "' | current='Answer' | next=':'\n",
      "❌ Not XML pattern - prev doesn't end with '<' or next doesn't start with '>'\n",
      "❌ No XML answer pattern found\n",
      "All format-specific methods failed, using universal answer search...\n",
      "Found answer keyword at position 1: ' answer'\n",
      "Found answer keyword at position 572: ' answers'\n",
      "Found answer letter 'A' at position 575\n",
      "Finish 800 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 10/22 [02:30<03:16, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad response detected at index 19 (4096 tokens), retrying...\n",
      "Successfully retried response 19\n",
      "Format-specific parsing failed for BASE, trying fallback methods...\n",
      "🔍 Starting FIXED XML answer search...\n",
      "📍 Found 'Answer' token at position 329\n",
      "📋 Context: prev='.\n",
      "\n",
      "' | current='Answer' | next=':'\n",
      "❌ Not XML pattern - prev doesn't end with '<' or next doesn't start with '>'\n",
      "❌ No XML answer pattern found\n",
      "All format-specific methods failed, using universal answer search...\n",
      "Found answer keyword at position 329: 'Answer'\n",
      "Found embedded answer letter 'A' in keyword token 'Answer' at position 329\n",
      "Finish 880 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 11/22 [02:47<03:02, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad response detected at index 0 (4096 tokens), retrying...\n",
      "Successfully retried response 0\n",
      "Finish 960 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 12/22 [03:04<02:47, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1040 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 13/22 [03:16<02:16, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1120 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 14/22 [03:20<01:34, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format-specific parsing failed for BASE, trying fallback methods...\n",
      "🔍 Starting FIXED XML answer search...\n",
      "📍 Found 'Answer' token at position 302\n",
      "📋 Context: prev=':\n",
      "\n",
      "' | current='Answer' | next=':'\n",
      "❌ Not XML pattern - prev doesn't end with '<' or next doesn't start with '>'\n",
      "❌ No XML answer pattern found\n",
      "All format-specific methods failed, using universal answer search...\n",
      "Found answer keyword at position 299: ' answer'\n",
      "Found embedded answer letter 'A' in token 'Answer' at position 302\n",
      "Finish 1200 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 15/22 [03:24<01:06,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1280 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 16/22 [03:28<00:48,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad response detected at index 41 (4096 tokens), retrying...\n",
      "Successfully retried response 41\n",
      "Finish 1360 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 17/22 [03:44<00:52, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1440 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 18/22 [03:49<00:34,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1520 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 19/22 [03:53<00:21,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1600 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 20/22 [03:57<00:12,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1680 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 21/22 [04:02<00:05,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1760 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [04:05<00:00, 11.16s/it]\n"
     ]
    }
   ],
   "source": [
    "save_path = f\"{save_dir}/{save_name}.jsonl\"\n",
    "results = await run_experiemnts(curr_queries, save_path, try_first_n=None, curr_output_format = curr_output_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmmlu_preparer.query_formats import (\n",
    "    get_current_queries,\n",
    "    InputFormat,\n",
    "    OutputFormat,\n",
    "    ShuffleMethod\n",
    ")\n",
    "for method in ['LONGEST_FIRST', 'SHORTEST_FIRST']:\n",
    "    for input_format in ['BASE', 'JSON', 'XML']:\n",
    "        for output_format in ['BASE', 'JSON_FULL', 'XML_FULL']:\n",
    "            \n",
    "            # BASE, JSON, XML\n",
    "            if input_format == 'BASE':\n",
    "                curr_input_format = InputFormat.BASE\n",
    "            elif input_format == 'JSON':\n",
    "                curr_input_format = InputFormat.JSON\n",
    "            elif input_format == 'XML':\n",
    "                curr_input_format = InputFormat.XML\n",
    "\n",
    "            if output_format == 'BASE':\n",
    "                # BASE, JSON_FULL, XML_FULL\n",
    "                curr_output_format = OutputFormat.BASE\n",
    "            elif output_format == 'JSON_FULL':  \n",
    "                curr_output_format = OutputFormat.JSON_FULL\n",
    "            elif output_format == 'XML_FULL':\n",
    "                curr_output_format = OutputFormat.XML_FULL\n",
    "\n",
    "            if method == 'LONGEST_FIRST':\n",
    "                # DEFAULT, REVERSE, LONGEST_FIRST, SHORTEST_FIRST, MOST_KANA_RATIO, FEWEST_KANA_RATIO\n",
    "                curr_shuffle_method = ShuffleMethod.LONGEST_FIRST\n",
    "            elif method == 'SHORTEST_FIRST':\n",
    "                # DEFAULT, REVERSE, LONGEST_FIRST, SHORTEST_FIRST, MOST_KANA_RATIO, FEWEST_KANA_RATIO\n",
    "                curr_shuffle_method = ShuffleMethod.SHORTEST_FIRST\n",
    "\n",
    "\n",
    "                input_format_save_name = curr_input_format.value.lower().replace(\"_\", \"-\")\n",
    "                output_format_save_name = curr_output_format.name.lower().replace(\"_\", \"-\")\n",
    "                shuffle_method_save_name = curr_shuffle_method.name.lower().replace(\"_\", \"-\")\n",
    "\n",
    "                language_name = curr_language.lower().replace(\"_\", \"-\")\n",
    "\n",
    "                save_name = f\"{model_name}_{language_name}_{input_format_save_name}_input_{output_format_save_name}_output_{shuffle_method_save_name}_shuffle\"\n",
    "                save_name\n",
    "\n",
    "                curr_queries = get_current_queries(mmmlu_subset,\n",
    "                                                dataset_language_enum,\n",
    "                                                chosen_subtasks,\n",
    "                                                curr_input_format,\n",
    "                                                curr_output_format,\n",
    "                                                curr_shuffle_method,\n",
    "                                                )\n",
    "\n",
    "\n",
    "                save_path = f\"{save_dir}/{save_name}.jsonl\"\n",
    "                results = await run_experiemnts(curr_queries, save_path, try_first_n=None, curr_output_format = curr_output_format)\n",
    "                await asyncio.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "save_path = f\"{save_dir}/{save_name}.jsonl\"\n",
    "target_save_path = Path(save_path)\n",
    "with target_save_path.open('r', encoding='utf-8') as file:\n",
    "    result_dicts = [json.loads(line) for line in file if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ac99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_dicts[1]['kwargs']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1144e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = [result['kwargs']['content'] for result in result_dicts]\n",
    "output_answer = []\n",
    "none_answer_indice = []\n",
    "none_answer_output = []\n",
    "for idx, output in enumerate(output_text):\n",
    "    extracted_answer = extract_answer_from_response(output)\n",
    "    output_answer.append(extracted_answer)\n",
    "    if extracted_answer is None:\n",
    "        none_answer_indice.append(idx)\n",
    "        none_answer_output.append(output)\n",
    "        #print(idx)\n",
    "        print(f\"{idx}:\\n{output = }\\n\")\n",
    "\n",
    "print(f\"{len(none_answer_indice) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "output_tokens_list = [result['kwargs']['usage_metadata']['total_tokens'] for result in result_dicts]\n",
    "print(np.argsort(-np.array(output_tokens_list)).tolist())\n",
    "print(np.sort(-np.array(output_tokens_list)).tolist())\n",
    "print(f\"median: {np.median(output_tokens_list)}\")\n",
    "print(f\"mean: {np.mean(output_tokens_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_dicts[7]['kwargs']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.output_parsers.json import JsonOutputParser\n",
    "# import re\n",
    "# from tqdm.auto import tqdm\n",
    "# parser = JsonOutputParser()\n",
    "\n",
    "# for result in tqdm(result_dicts):\n",
    "#     #try:\n",
    "#     string = result['kwargs']['content']\n",
    "\n",
    "#     def escape_single_backslash(match):\n",
    "#         c = match.group(0)\n",
    "#         return c.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "\n",
    "#     # ChatGPT\n",
    "#     string = re.sub(r'(?<!\\\\)\\\\(?![\\\\ntbrf\"u])', escape_single_backslash, string)\n",
    "\n",
    "#     x = (parser.parse(string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
